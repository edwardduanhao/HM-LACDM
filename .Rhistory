# pi
alpha_prior <- torch_ones(L, dtype = torch_float()) # Dirichlet prior
alpha <- torch_ones(L, dtype = torch_float())
alpha_trace <- torch_zeros(max_iter, L) # Trace of alpha
# Z
E_Z <- torch_ones(I, indT, L) / L
E_Z_inter <- torch_ones(I, indT, L, L) / (L * L)
E_Z_trace <- torch_zeros(max_iter, I, indT, L) # Trace of Z
# xi
xi <- torch_zeros(J, L)
xi_trace <- torch_zeros(max_iter, J, L) # Trace of xi
# --------------------- Main loop --------------------- #
# for (iter_ in seq(max_iter)){
#
#
#
#
# }
iter_ <- 1
# Update beta
update_beta(
Y = Y,
K = K,
M_beta_prior = M_beta_prior,
V_beta_prior = V_beta_prior,
Delta_matrices = Delta_matrices,
E_Z = E_Z,
xi = xi
) %->% c(M_beta, V_beta)
# Track the trace of beta
for (j in seq(J)) {
beta_trace[[j]][iter_, ] <- M_beta[[j]]
}
# Update xi
update_xi(
M_beta = M_beta,
V_beta = V_beta,
Delta_matrices = Delta_matrices
) %->% xi
xi_trace[iter_, , ] <- xi
M_beta
V_beta
M_beta
# Z
# E_Z <- torch_ones(I, indT, L) / L
E_Z <- torch_randn(I, indT, L) |> torch_softmax(dim = 3)
# Z
# E_Z <- torch_ones(I, indT, L) / L
E_Z <- torch_randn(I, indT, L) |> nn_softmax(dim = 3)
# Z
# E_Z <- torch_ones(I, indT, L) / L
E_Z <- torch_randn(I, indT, L) |> nnf_softmax(dim = 3)
E_Z[1,1]
E_Z[1,1]$sum()
E_Z[1,2]$sum()
source("utils.R")
source("utils_vb.R")
library(cli)
library(zeallot)
library(torch)
# VB_TDCM_fit_IW <- function(data,
#                            interact = TRUE,
#                            max_iter = 100){
#
# }
max_iter <- 100
# Print the number of threads Torch is using
print(paste("Torch is using", torch_get_num_threads(), "threads"))
# Setup a timer
start <- Sys.time()
# --------------------- Retrieve and format data --------------------- #
# Response matrix Y, shape (N, indT, J)
Y <- torch_tensor(data$Y)
# Number of observations, number of time points, number of items
c(I, indT, J) %<-% Y$shape
# Number of attributes
K <- data$K
L <- 2^K
# Q-matrix
Q_mat <- data$Q_matrix
# Generate the delta matrix for each item
Delta_matrices <- lapply(seq(J), \(j) build_delta(
q = rep(1, K),
interact = FALSE
) |>
torch_tensor())
beta_dim <- rep(K + 1, J)
# --------------------- Parameter initialization --------------------- #
# beta
M_beta_prior <- lapply(beta_dim, \(d) torch_zeros(d)) # Mean
M_beta <- lapply(beta_dim, \(d) torch_zeros(d)) # Mean
V_beta_prior <- lapply(beta_dim, \(d) torch_eye(d) * 1e2) # Covariance matrix
V_beta <- lapply(beta_dim, \(d) torch_eye(d) * 1e2) # Covariance matrix
beta_trace <- lapply(beta_dim, \(d) torch_zeros(max_iter, d)) # Trace of beta
# tau
omega_prior <- torch_ones(L, L, dtype = torch_float()) # Dirichlet priors
omega <- torch_ones(L, L, dtype = torch_float())
omega_trace <- torch_zeros(max_iter, L, L) # Trace of omega
# pi
alpha_prior <- torch_ones(L, dtype = torch_float()) # Dirichlet prior
alpha <- torch_ones(L, dtype = torch_float())
alpha_trace <- torch_zeros(max_iter, L) # Trace of alpha
# Z
# E_Z <- torch_ones(I, indT, L) / L
E_Z <- torch_randn(I, indT, L) |> nnf_softmax(dim = 3)
E_Z_inter <- torch_ones(I, indT, L, L) / (L * L)
E_Z_trace <- torch_zeros(max_iter, I, indT, L) # Trace of Z
# xi
xi <- torch_zeros(J, L)
xi_trace <- torch_zeros(max_iter, J, L) # Trace of xi
# --------------------- Main loop --------------------- #
# for (iter_ in seq(max_iter)){
#
#
#
#
# }
iter_ <- 1
# Update beta
update_beta(
Y = Y,
K = K,
M_beta_prior = M_beta_prior,
V_beta_prior = V_beta_prior,
Delta_matrices = Delta_matrices,
E_Z = E_Z,
xi = xi
) %->% c(M_beta, V_beta)
# Track the trace of beta
for (j in seq(J)) {
beta_trace[[j]][iter_, ] <- M_beta[[j]]
}
# Update xi
update_xi(
M_beta = M_beta,
V_beta = V_beta,
Delta_matrices = Delta_matrices
) %->% xi
xi_trace[iter_, , ] <- xi
M_beta
xi
200000/1000*7
E_Z_inter$sum(1,2)
E_Z_inter$sum(c(1,2))
1000/31.25
E_Z$shape
E_Z[,1,]
library(torch)
library(zeallot)
source("utils.R")
# Data generate function for simulation study
data_generate <- function(I, # Number of respondents
K, # Number of latent attributes
J, # Number of items
indT, # Number of time points
N_dataset = 1, # Number of datasets to generate
seed = NULL, # Seed for reproducibility
Q_mat = NULL # Generate a random Q-matrix if NULL
) {
# If the seed is not NULL, set the seed; otherwise, we do not set the seed
if (!is.null(seed)) {
torch_manual_seed(seed = seed)
}
# Generate Q-Matrix if it is not given
if (is.null(Q_mat)) {
Q_mat <- torch_randint(
low = 1, # inclusive
high = 2^K, # exclusive
size = J
) |>
as_array() |>
sapply(
\(int_class) intToBin(
x = int_class,
d = K
)
) |>
t()
}
# Generate the delta matrix for each item
Delta_matrices <- lapply(seq(J), \(j) build_delta(
q = Q_mat[j, ],
interact = FALSE
) |>
torch_tensor())
# Generate the beta vector for each item
beta <- rowSums(Q_mat) |>
lapply(\(s) build_beta(
K = s
) |>
torch_tensor())
# Initial distribution of the latent attributes
pii <- torch_ones(2^K, dtype = torch_float()) / 2^K
# Transition probability
kernel_mat <- matrix(c(0.4, 0.6, 0.1, 0.9), nrow = 2, byrow = TRUE)
omega <- torch_ones(2^K, 2^K, dtype = torch_float())
for (l_prev in seq(2^K)){
profile_prev <- intToBin(l_prev - 1, d = K)
for (l_after in seq(2^K)){
profile_after <- intToBin(l_after - 1, d = K)
for (k in seq(K)){
omega[l_prev, l_after] <- omega[l_prev, l_after] *
kernel_mat[profile_prev[k] + 1, profile_after[k] + 1]
}
}
}
# Sample the latent attribute profiles over time
int_class <- array(0, c(I, indT))
# t = 1
int_class[,1] <- as_array(torch_multinomial(pii, I, replacement=TRUE))
# t > 1
for (t in seq(2, indT)){
for (l in seq(2^K)){
index <- int_class[, t - 1] == l
int_class[index, t] <- as_array(torch_multinomial(omega[l, ], as.integer(sum(index)), replacement = TRUE))
}
}
# Generate Response Matrix Y
Y <- torch_zeros(N_dataset, I, indT, J)
for (t in 1:indT) {
for (j in 1:J) {
delta_matrix_t <- Delta_matrices[[j]][as.numeric(int_class[,t]), ]
Y_sampler <- (delta_matrix_t %@% beta[[j]]) |> torch_sigmoid() |> distr_bernoulli()
Y[, , t, j] <- Y_sampler$sample(N_dataset)
}
}
if (N_dataset == 1) {
Y <- Y[1, , , ]
}
# Convert int_class to profiles_mat
profiles_mat <- array(0, c(I, indT, K))
for (t in seq(indT)) {
for (i in seq(I)) {
profiles_mat[i, t, ] <- intToBin(int_class[i, t] - 1, d = K)
}
}
list(
"Y" = as_array(Y),
"K" = K,
"profiles_mat" = profiles_mat,
"profiles_index" = int_class,
"beta" = lapply(beta, \(beta_vec) as_array(beta_vec)),
"Q_matrix" = Q_mat,
"folder_name" = paste("I", I, "K", K, "J", J, "T", indT, sep = "_")
)
}
if (TRUE) {
Q_mat <- as.matrix(read.table("Q_Matrix/Q_3.txt"))
data <- data_generate(
I = 1000,
K = 3,
J = 21,
indT = 2,
N_dataset = 1,
seed = 2025,
Q_mat = Q_mat
)
}
source("utils.R")
source("utils_vb.R")
library(cli)
library(zeallot)
library(torch)
# VB_TDCM_fit_IW <- function(data,
#                            interact = TRUE,
#                            max_iter = 100){
#
# }
max_iter <- 100
# Print the number of threads Torch is using
print(paste("Torch is using", torch_get_num_threads(), "threads"))
# Setup a timer
start <- Sys.time()
# --------------------- Retrieve and format data --------------------- #
# Response matrix Y, shape (N, indT, J)
Y <- torch_tensor(data$Y)
# Number of observations, number of time points, number of items
c(I, indT, J) %<-% Y$shape
# Number of attributes
K <- data$K
L <- 2^K
# Q-matrix
Q_mat <- data$Q_matrix
# Generate the delta matrix for each item
Delta_matrices <- lapply(seq(J), \(j) build_delta(
q = rep(1, K),
interact = FALSE
) |>
torch_tensor())
beta_dim <- rep(K + 1, J)
# --------------------- Parameter initialization --------------------- #
# beta
M_beta_prior <- lapply(beta_dim, \(d) torch_zeros(d)) # Mean
M_beta <- lapply(beta_dim, \(d) torch_zeros(d)) # Mean
V_beta_prior <- lapply(beta_dim, \(d) torch_eye(d) * 1e2) # Covariance matrix
V_beta <- lapply(beta_dim, \(d) torch_eye(d) * 1e2) # Covariance matrix
beta_trace <- lapply(beta_dim, \(d) torch_zeros(max_iter, d)) # Trace of beta
# tau
omega_prior <- torch_ones(L, L, dtype = torch_float()) # Dirichlet priors
omega <- torch_ones(L, L, dtype = torch_float())
omega_trace <- torch_zeros(max_iter, L, L) # Trace of omega
# pi
alpha_prior <- torch_ones(L, dtype = torch_float()) # Dirichlet prior
alpha <- torch_ones(L, dtype = torch_float())
alpha_trace <- torch_zeros(max_iter, L) # Trace of alpha
# Z
E_Z <- torch_ones(I, indT, L) / L
# E_Z <- torch_randn(I, indT, L) |> nnf_softmax(dim = 3)
E_Z_inter <- torch_ones(I, indT, L, L) / (L * L)
E_Z_trace <- torch_zeros(max_iter, I, indT, L) # Trace of Z
# xi
xi <- torch_zeros(J, L)
xi_trace <- torch_zeros(max_iter, J, L) # Trace of xi
# --------------------- Main loop --------------------- #
# for (iter_ in seq(max_iter)){
#
#
#
#
# }
iter_ <- 1
# Update beta
update_beta(
Y = Y,
K = K,
M_beta_prior = M_beta_prior,
V_beta_prior = V_beta_prior,
Delta_matrices = Delta_matrices,
E_Z = E_Z,
xi = xi
) %->% c(M_beta, V_beta)
# Track the trace of beta
for (j in seq(J)) {
beta_trace[[j]][iter_, ] <- M_beta[[j]]
}
# Update xi
update_xi(
M_beta = M_beta,
V_beta = V_beta,
Delta_matrices = Delta_matrices
) %->% xi
xi_trace[iter_, , ] <- xi
# Update omega
update_omega(
omega_prior = omega_prior,
E_Z_inter = E_Z_inter
) %->% omega
omega_trace[iter_, , ] <- omega
# Update alpha
update_alpha(
alpha_prior = alpha_prior,
E_Z = E_Z
) %->% alpha
alpha
torch_digamma(1)
alpha
alpha$sum(-1)
E_log_pi <- function(alpha){
return(torch_digamma(alpha) - torch_digamma(alpha$sum(-1)))
}
E_log_pi
E_log_pi(alpha)
exp(-2)
omega
torch_digamma(omega)
omega$sum(1)
# Compute the expectation of log(omega)
E_log_omega <- function(omega){
return(torch_digamma(omega) - torch_digamma(omega$sum(1)))
}
E_log_omega(omega)
knitr::opts_chunk$set(echo = TRUE)
library(MGMM)
library(ggplot2)
library(dplyr)
library(maximin)
get_distance <- function(a, b) {
# a: num_users * latent_dim
# b: num_items * latent_dim
# return: num_users * num_items
# calculate the Euclidean distance between two matrices
a_norm <- rowSums(a^2) # J * 1
b_norm <- rowSums(b^2) # I * 1
a_b <- a %*% t(b) # J * I
distance <- sqrt(outer(a_norm, b_norm, FUN = "+") - 2 * a_b)
return (distance)
}
# Latent positions for items, based on Gaussian mixture model
get_items_latent <- function(num_items, num_clusters, latent_dim, sigma_latent,
seed = 2025){
# num_items: number of items
# num_clusters: number of clusters
# latent_dim: dimension of latent space
# sigma_latent: standard deviation of the Gaussian noise
set.seed(seed)
# generate cluster means, spread out based on sigma_latent
# cluster_centers <- lapply(1:num_clusters, function(i) {
#   as.vector(runif(latent_dim, -1, 1) * 6 * sigma_latent^2)
#   })
norm_rows <- function(M){sqrt(rowSums(M^2))}
S <- matrix(rnorm(num_clusters * 10 * latent_dim), ncol = latent_dim)
S <- 2 * sigma_latent * S / norm_rows(S)
best <- maximin.cand(n = num_clusters, Xcand = S, Tmax = nrow(S))
centers <- S[best$inds, ]
cluster_centers <- list()
for (i in 1:num_clusters) {
cluster_centers[[i]] <- centers[i, ]
}
# Generate random cluster assignments for items
gmm_data <- rGMM(
n = num_items,
d = latent_dim,
k = num_clusters,
means = cluster_centers,
pi = rep(1/num_clusters, num_clusters), # equally weighted clusters
miss = 0,
covs = diag(1/8 * sigma_latent**2, latent_dim))
return (list('data' = gmm_data, 'centers' = cluster_centers))
}
# Set parameters
num_clusters <- 4
latent_dim <- 2
sigma_latent <- 1
num_items <- 200
set.seed(2025)
simulated_data <- get_items_latent(num_items, num_clusters, latent_dim, sigma_latent)
gmm_data <- simulated_data$data
cluster_centers <- simulated_data$centers
centers_df <- do.call(rbind, cluster_centers)
centers_df <- as.data.frame(centers_df)
colnames(centers_df) <- c("x", "y")
centers_df$cluster <- factor(1:nrow(centers_df))
# Plot
ggplot(centers_df, aes(x = x, y = y, color = cluster)) +
geom_point(size = 5, shape = 4, stroke = 2) +  # shape 4 = "x"
coord_equal() +
labs(title = "Cluster Centers", x = "Latent Dimension 1", y = "Latent Dimension 2") +
theme_minimal() +
theme(legend.position = "none")
# Set parameters
row_name <- rownames(gmm_data)
gmm_df <- as.data.frame(gmm_data[, 1:2]) %>%
mutate(cluster = row_name) %>%
select(cluster, everything())
ggplot(gmm_df, aes(x = y1, y = y2, color = cluster)) +
geom_point(size = 3, alpha = 0.8) +  # Plot individual points
geom_density_2d() +  # Add contour lines to show density
labs(title = "GMM Cluster Visualization with Density",
x = "Latent Dimension 1",
y = "Latent Dimension 2") +
theme_minimal() +
theme(legend.title = element_blank())
data_generate <- function(
num_users,
num_items,
num_clusters = 4,
sigma_alpha = 1.0,
sigma_beta = 1.0,
sigma_latent = 2.0,
gamma = 1,
latent_dim = 2,
seed = 2025
) {
# Build num_users x num_items matrix
set.seed(seed)
alpha <- rnorm(num_users, 1, sigma_alpha) # + (sample(0:1, num_users, replace = T, prob = c(0.5, 0.5)) * 2 - 1) * 1
beta <- rnorm(num_items, 1, sigma_beta) # + (sample(0:1, num_items, replace = T, prob = c(0.5, 0.5)) * 2 - 1) * 1
# generate latent positions
a <- matrix(rnorm(num_users * latent_dim, 0, sigma_latent), nrow = num_users)
b <- get_items_latent(num_items, num_clusters, latent_dim, sigma_latent)$data
distance <- get_distance(a, b)
linear_predictor <- outer(alpha, beta, FUN = "+") - gamma * distance
sigmoid <- function(x) {
return (exp(x) / (1 + exp(x)))
}
linear_predictor_sigmoid <- sigmoid(linear_predictor)
Y <- matrix(runif(num_users * num_items, 0, 1), nrow = num_users)
Y <- ifelse(Y < linear_predictor_sigmoid, 1, 0) # J * I
return (list(
Y = Y,
alpha = alpha,
beta = beta,
a = a,
b = b,
distance = distance,
logits = linear_predictor
))
}
num_users <- 500
num_items <- 400
sigma_alpha <- 0.1
sigma_beta <- 0.1
sigma_latent <- 1
gamma <- 1
latent_dim <- 2
num_clusters <- 4
data <- data_generate(
num_users = num_users,
num_items = num_items,
num_clusters = num_clusters,
sigma_alpha = sigma_alpha,
sigma_beta = sigma_beta,
sigma_latent = sigma_latent,
gamma = gamma,
latent_dim,
seed = 2025
)
hist(data$logits)
mean(data$Y)
plot(data$b[, 1], data$b[, 2], col = "red", pch = 19, xlim = c(-3, 3), ylim = c(-3, 3), xlab = "x", ylab = "y", main = "Item Latent Positions")
points(data$a[, 1], data$a[, 2], col = "blue", pch = 19, xlab = "x", ylab = "y", main = "User Latent Positions")
write.csv(data$Y, "simulated_data.csv", row.names = FALSE)
